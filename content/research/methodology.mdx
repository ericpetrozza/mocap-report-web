# Research Methodology

## Research Scope

This analysis synthesizes data from seven distinct research activities conducted between October and November 2025.

### Primary Research
- **Live stakeholder interviews:** 10 sessions, 12 participants (leadership, regional directors, IT)
- **Written stakeholder feedback:** 16 frontline team members (CS, inside sales, sales support)
- **Total stakeholders:** 28 unique voices across US, UK/EU, Mexico, China

### Product Analysis
- **MOCAP catalog:** 85 series, 2,195 SKUs, ~66,000 variants
- **Cleartec catalog:** 21 series, 328 SKU variants
- **Beckett catalog:** 18 series, 440 SKU variants
- **X-Treme Tape catalog:** 3 product types, 24 SKUs

### Behavioral Research
- **CrazyEgg heatmap analysis:** 13 pages across three brands
- **Search log analysis:** 1,000 queries sampled
- **Google Analytics audit:** Full year, three brand properties

### Market Research
- **Competitor analysis:** 14 B2B industrial suppliers
- **MoSCoW prioritization:** 24 stakeholders, 900+ feature ratings

### Supporting Documents
- Project brief and technical specifications
- Persona development (five primary buyer types)
- Stakeholder research synthesis

---

## Stakeholder Profile

### By Role (28 total)

```
Leadership/Directors:        12 participants (43%)
Customer Service:            8 participants (29%)
Sales (Inside/Field):        9 participants (32%)
IT/Operations:              3 participants (11%)
Marketing:                  1 participant (4%)

*Some participants hold multiple roles
```

### By Region

```
United States:              14 participants (50%)
United Kingdom/Europe:       8 participants (29%)
Mexico:                     2 participants (7%)
China:                      1 participant (4%)
Regional Directors:         3 participants (11%)
```

### By Customer Proximity

```
Direct customer interaction:  16 participants (57%)
Leadership/strategic:        12 participants (43%)
```

This split matters. Customer-facing staff rated stock visibility and data accuracy significantly higher than leadership, revealing daily operational realities that executives don't directly experience.

---

## Data Collection Methods

### Live Interviews (60-90 minutes each)
- Open-ended discussion of pain points and opportunities
- Screen sharing through current website workflows
- Feature prioritization using 1-5 impact scale
- Technical constraint discussion with IT team

### Written Feedback Surveys
- Structured around daily operational challenges
- Top 5 priority feature selection
- Detailed pain point descriptions
- Feature impact rating (personal and customer dimensions)

### MoSCoW Prioritization
- 38-50 features rated by each stakeholder
- Two dimensions: Personal Impact (1-5) and Customer Impact (1-5)
- High priority threshold: 4.0+ on both dimensions
- Critical priority: 4.5+ on both, or 4.0+ with 75%+ consensus

### Product Catalog Audits
- Completeness analysis (descriptions, specs, dimensions)
- Data quality scoring across 15+ attributes
- Taxonomy structure evaluation
- Variant complexity documentation
- Cross-reference opportunity identification

### Behavioral Analysis
- CrazyEgg click heatmaps: Click patterns, dead clicks, rage clicks
- Scroll depth analysis
- Mobile vs desktop behavior comparison
- Search log parsing: Query patterns, error rates, latency

### Competitive Analysis
- Feature inventory across 14 competitors
- UX pattern documentation
- Pricing and positioning analysis
- Search and navigation comparison

---

## What Makes This Research Reliable

### Convergence across sources
The same three failures emerged independently from live interviews, written feedback, analytics data, and behavioral analysis. When 68% of stakeholders mention search dysfunction without prompting, and search logs show 100% error rates, that's not coincidence.

### Quantified impacts
40-50% European order cancellations. 70 manual orders daily. 15+ calls per rep for search help. 50%+ CS calls for order status. These aren't estimates. They're measured operational realities.

### Cross-regional validation
US, Europe, Mexico, and China teams independently identified the same core issues. Regional variations exist (Europe's data accuracy crisis is more severe, China has zero online orders), but the fundamental problems transcend geography.

### Technical evidence
Search logs provide 1,000 real queries with timestamps, error codes, and latency. Google Analytics shows 27K search page views despite broken functionality. CrazyEgg heatmaps reveal 11.6% dead clicks on table headers users expect to be sortable.

### Stakeholder consensus
Out of 900+ individual feature ratings, five features achieved 80-100% consensus. This level of agreement across different roles, regions, and customer proximity levels is statistically significant.

---

## Research Limitations

### Missing voices
The European Operations Director (Honorata), China Operations Director (Linda), and Cleartec Director (Dave) participated in interviews but didn't complete written MoSCoW surveys. Their feature prioritization data would likely strengthen regional and brand-specific insights.

### Sample size per brand
Beckett and X-Treme Tape had limited stakeholder representation compared to MOCAP. Product catalog analysis partially compensates, but fewer operational voices from these brands means some nuance may be missed.

### No direct customer research
This analysis relies on stakeholder observations of customer behavior rather than direct customer interviews. CrazyEgg and analytics data provide some behavioral validation, but customer voice is filtered through internal perspectives.

### Time constraints
Search log analysis covered 1,000 queries (representative sample) rather than exhaustive analysis. CrazyEgg captured 13 pages rather than full site coverage. These samples reveal patterns but may miss edge cases.
